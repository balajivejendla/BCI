{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import optimizers\n",
        "from dataset import load_dataset, load_labels, split_data, format_labels\n",
        "import variables as v\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Model for EEG Stress Detection\n",
        "\n",
        "This notebook implements a Transformer neural network for classifying EEG stress levels.\n",
        "Transformers use self-attention mechanisms to capture long-range dependencies in time-series data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - using filtered_data\n",
        "data_type = \"wt_filtered\"\n",
        "test_type = \"Arithmetic\"\n",
        "print(f\"Data type: {data_type}\")\n",
        "print(f\"Test type: {test_type}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "dataset_ = load_dataset(data_type=data_type, test_type=test_type)\n",
        "dataset = split_data(dataset_, v.SFREQ)\n",
        "\n",
        "print(f\"Dataset shape after splitting: {dataset.shape}\")\n",
        "print(f\"Shape breakdown: (trials={dataset.shape[0]}, epochs={dataset.shape[1]}, channels={dataset.shape[2]}, timepoints={dataset.shape[3]})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load labels\n",
        "label_ = load_labels()\n",
        "label = format_labels(label_, test_type=test_type, epochs=dataset.shape[1])\n",
        "\n",
        "print(f\"Label shape: {label.shape}\")\n",
        "print(f\"Label distribution: {np.bincount(label.astype(int))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Reshape Data for Transformer\n",
        "\n",
        "Transformers require data in the format: (samples, sequence_length, features)\n",
        "- Each epoch (1 second) will be one sample\n",
        "- Sequence length = 128 (timepoints per second)\n",
        "- Features = 32 (EEG channels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reshape data for Transformer: (trials, epochs, channels, timepoints) -> (samples, sequence_length, features)\n",
        "n_trials, n_epochs, n_channels, n_timepoints = dataset.shape\n",
        "\n",
        "# Reshape to (n_trials * n_epochs, n_timepoints, n_channels)\n",
        "X = dataset.reshape(n_trials * n_epochs, n_timepoints, n_channels)\n",
        "y = label.reshape(-1)\n",
        "\n",
        "print(f\"Reshaped X shape: {X.shape} (samples, sequence_length, features)\")\n",
        "print(f\"Reshaped y shape: {y.shape}\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Sequence length: {X.shape[1]}\")\n",
        "print(f\"Features per timestep: {X.shape[2]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Split Data into Train, Validation, and Test Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train/test (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Split train into train/validation (75/25 of train set)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Normalize Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize data: fit on training set, transform all sets\n",
        "n_samples_train, n_seq_len, n_features = X_train.shape\n",
        "X_train_reshaped = X_train.reshape(-1, n_features)\n",
        "X_val_reshaped = X_val.reshape(-1, n_features)\n",
        "X_test_reshaped = X_test.reshape(-1, n_features)\n",
        "\n",
        "# Fit scaler on training data\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train_reshaped)\n",
        "\n",
        "# Transform all sets\n",
        "X_train_scaled = scaler.transform(X_train_reshaped).reshape(X_train.shape)\n",
        "X_val_scaled = scaler.transform(X_val_reshaped).reshape(X_val.shape)\n",
        "X_test_scaled = scaler.transform(X_test_reshaped).reshape(X_test.shape)\n",
        "\n",
        "print(f\"Scaled training data shape: {X_train_scaled.shape}\")\n",
        "print(f\"Data range after scaling: [{X_train_scaled.min():.3f}, {X_train_scaled.max():.3f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert labels to categorical for binary classification\n",
        "y_train_cat = to_categorical(y_train, num_classes=v.N_CLASSES)\n",
        "y_val_cat = to_categorical(y_val, num_classes=v.N_CLASSES)\n",
        "y_test_cat = to_categorical(y_test, num_classes=v.N_CLASSES)\n",
        "\n",
        "print(f\"Categorical labels shape: {y_train_cat.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build Transformer Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer Encoder Block\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.3):\n",
        "    \"\"\"\n",
        "    Creates a transformer encoder block with multi-head self-attention.\n",
        "    \"\"\"\n",
        "    # Multi-head self-attention\n",
        "    attention_output = MultiHeadAttention(\n",
        "        key_dim=head_size, \n",
        "        num_heads=num_heads, \n",
        "        dropout=dropout\n",
        "    )(inputs, inputs)\n",
        "    attention_output = Dropout(dropout)(attention_output)\n",
        "    x = LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
        "    \n",
        "    # Feed-forward network\n",
        "    ffn_output = Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
        "    ffn_output = Dropout(dropout)(ffn_output)\n",
        "    outputs = LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
        "    \n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear any existing models\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# Model parameters\n",
        "sequence_length = X_train_scaled.shape[1]\n",
        "num_features = X_train_scaled.shape[2]\n",
        "head_size = 64\n",
        "num_heads = 4\n",
        "ff_dim = 128\n",
        "num_transformer_blocks = 2\n",
        "mlp_units = [64, 32]\n",
        "mlp_dropout = 0.4\n",
        "dropout = 0.3\n",
        "\n",
        "# Input layer\n",
        "inputs = Input(shape=(sequence_length, num_features))\n",
        "\n",
        "# Transformer blocks\n",
        "x = inputs\n",
        "for _ in range(num_transformer_blocks):\n",
        "    x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "# Global average pooling\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "# MLP head\n",
        "for dim in mlp_units:\n",
        "    x = Dense(dim, activation=\"relu\")(x)\n",
        "    x = Dropout(mlp_dropout)(x)\n",
        "\n",
        "# Output layer\n",
        "outputs = Dense(v.N_CLASSES, activation=\"softmax\", name=\"output\")(x)\n",
        "\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Display model architecture\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=15,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train_cat,\n",
        "    batch_size=32,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val_scaled, y_val_cat),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluate Model Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_cat, verbose=0)\n",
        "print(\"=\" * 60)\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred_proba = model.predict(X_test_scaled, verbose=0)\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "y_true = np.argmax(y_test_cat, axis=1)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 60)\n",
        "print(metrics.classification_report(y_true, y_pred, \n",
        "                                    target_names=['Low Stress', 'High Stress']))\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CONFUSION MATRIX\")\n",
        "print(\"=\" * 60)\n",
        "cm = metrics.confusion_matrix(y_true, y_pred)\n",
        "print(cm)\n",
        "print(\"\\nConfusion Matrix Breakdown:\")\n",
        "print(f\"True Negatives (Low Stress correctly predicted):  {cm[0][0]}\")\n",
        "print(f\"False Positives (Low Stress predicted as High):  {cm[0][1]}\")\n",
        "print(f\"False Negatives (High Stress predicted as Low):  {cm[1][0]}\")\n",
        "print(f\"True Positives (High Stress correctly predicted): {cm[1][1]}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Plot Training History\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Model and Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate best validation accuracy if not already calculated\n",
        "if 'best_val_acc' not in locals():\n",
        "    best_val_acc = max(history.history['val_accuracy'])\n",
        "    best_val_epoch = np.argmax(history.history['val_accuracy']) + 1\n",
        "\n",
        "# Save the trained model\n",
        "model.save('transformer_stress_model.h5')\n",
        "print(\"Model saved as 'transformer_stress_model.h5'\")\n",
        "\n",
        "# Save training history\n",
        "import pickle\n",
        "with open('transformer_training_history.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "print(\"Training history saved as 'transformer_training_history.pkl'\")\n",
        "\n",
        "# Save scaler for future use\n",
        "import joblib\n",
        "joblib.dump(scaler, 'transformer_scaler.pkl')\n",
        "print(\"Scaler saved as 'transformer_scaler.pkl'\")\n",
        "\n",
        "# Save results summary\n",
        "results_summary = {\n",
        "    'test_accuracy': float(test_accuracy),\n",
        "    'test_loss': float(test_loss),\n",
        "    'best_val_accuracy': float(best_val_acc),\n",
        "    'best_val_epoch': int(best_val_epoch),\n",
        "    'confusion_matrix': cm.tolist(),\n",
        "    'classification_report': metrics.classification_report(y_true, y_pred, \n",
        "                                                          target_names=['Low Stress', 'High Stress'], \n",
        "                                                          output_dict=True)\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('transformer_results.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=4)\n",
        "print(\"Results summary saved as 'transformer_results.json'\")\n",
        "print(\"\\nAll files saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Plot accuracy\n",
        "ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "ax1.set_title('Model Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Plot loss\n",
        "ax2.plot(history.history['loss'], label='Training Loss')\n",
        "ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
        "ax2.set_title('Model Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print best validation accuracy\n",
        "best_val_acc = max(history.history['val_accuracy'])\n",
        "best_val_epoch = np.argmax(history.history['val_accuracy']) + 1\n",
        "print(f\"\\nBest Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%) at epoch {best_val_epoch}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
