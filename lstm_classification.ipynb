{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from dataset import load_dataset, load_labels, split_data, format_labels\n",
        "import variables as v\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM Model for EEG Stress Detection\n",
        "\n",
        "This notebook implements an LSTM (Long Short-Term Memory) neural network for classifying EEG stress levels.\n",
        "LSTM is well-suited for time-series data like EEG signals as it can capture temporal dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "data_type = \"ica_filtered\"\n",
        "test_type = \"Arithmetic\"\n",
        "print(f\"Data type: {data_type}\")\n",
        "print(f\"Test type: {test_type}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "dataset_ = load_dataset(data_type=data_type, test_type=test_type)\n",
        "dataset = split_data(dataset_, v.SFREQ)\n",
        "\n",
        "print(f\"Dataset shape after splitting: {dataset.shape}\")\n",
        "print(f\"Shape breakdown: (trials={dataset.shape[0]}, epochs={dataset.shape[1]}, channels={dataset.shape[2]}, timepoints={dataset.shape[3]})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load labels\n",
        "label_ = load_labels()\n",
        "label = format_labels(label_, test_type=test_type, epochs=dataset.shape[1])\n",
        "\n",
        "print(f\"Label shape: {label.shape}\")\n",
        "print(f\"Label distribution: {np.bincount(label.astype(int))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Reshape Data for LSTM\n",
        "\n",
        "LSTM requires data in the format: (samples, timesteps, features)\n",
        "- Each epoch (1 second) will be one sample\n",
        "- Timesteps = 128 (samples per second)\n",
        "- Features = 32 (EEG channels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reshape data for LSTM: (trials, epochs, channels, timepoints) -> (samples, timesteps, features)\n",
        "# Each epoch becomes one sample with 128 timesteps and 32 features (channels)\n",
        "n_trials, n_epochs, n_channels, n_timepoints = dataset.shape\n",
        "\n",
        "# Reshape to (n_trials * n_epochs, n_timepoints, n_channels)\n",
        "X = dataset.reshape(n_trials * n_epochs, n_timepoints, n_channels)\n",
        "y = label.reshape(-1)\n",
        "\n",
        "print(f\"Reshaped X shape: {X.shape} (samples, timesteps, features)\")\n",
        "print(f\"Reshaped y shape: {y.shape}\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Timesteps per sample: {X.shape[1]}\")\n",
        "print(f\"Features per timestep: {X.shape[2]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Split Data into Train, Validation, and Test Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train/test (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Split train into train/validation (75/25 of train set)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Normalize Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize data: fit on training set, transform all sets\n",
        "# Reshape for scaling: (samples, timesteps, features) -> (samples * timesteps, features)\n",
        "n_samples_train, n_timesteps, n_features = X_train.shape\n",
        "X_train_reshaped = X_train.reshape(-1, n_features)\n",
        "X_val_reshaped = X_val.reshape(-1, n_features)\n",
        "X_test_reshaped = X_test.reshape(-1, n_features)\n",
        "\n",
        "# Fit scaler on training data\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train_reshaped)\n",
        "\n",
        "# Transform all sets\n",
        "X_train_scaled = scaler.transform(X_train_reshaped).reshape(X_train.shape)\n",
        "X_val_scaled = scaler.transform(X_val_reshaped).reshape(X_val.shape)\n",
        "X_test_scaled = scaler.transform(X_test_reshaped).reshape(X_test.shape)\n",
        "\n",
        "print(f\"Scaled training data shape: {X_train_scaled.shape}\")\n",
        "print(f\"Data range after scaling: [{X_train_scaled.min():.3f}, {X_train_scaled.max():.3f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert labels to categorical for binary classification\n",
        "y_train_cat = to_categorical(y_train, num_classes=v.N_CLASSES)\n",
        "y_val_cat = to_categorical(y_val, num_classes=v.N_CLASSES)\n",
        "y_test_cat = to_categorical(y_test, num_classes=v.N_CLASSES)\n",
        "\n",
        "print(f\"Categorical labels shape: {y_train_cat.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build LSTM Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear any existing models\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# Build LSTM model\n",
        "model = models.Sequential([\n",
        "    # Input layer\n",
        "    layers.Input(shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])),\n",
        "    \n",
        "    # First LSTM layer with dropout\n",
        "    layers.LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3),\n",
        "    \n",
        "    # Second LSTM layer with dropout\n",
        "    layers.LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.3),\n",
        "    \n",
        "    # Dense layer\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    \n",
        "    # Output layer\n",
        "    layers.Dense(v.N_CLASSES, activation='softmax', name='output')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Display model architecture\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=15,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train_cat,\n",
        "    batch_size=32,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val_scaled, y_val_cat),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluate Model Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_cat, verbose=0)\n",
        "print(\"=\" * 60)\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred_proba = model.predict(X_test_scaled, verbose=0)\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "y_true = np.argmax(y_test_cat, axis=1)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 60)\n",
        "print(metrics.classification_report(y_true, y_pred, \n",
        "                                    target_names=['Low Stress', 'High Stress']))\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CONFUSION MATRIX\")\n",
        "print(\"=\" * 60)\n",
        "cm = metrics.confusion_matrix(y_true, y_pred)\n",
        "print(cm)\n",
        "print(\"\\nConfusion Matrix Breakdown:\")\n",
        "print(f\"True Negatives (Low Stress correctly predicted):  {cm[0][0]}\")\n",
        "print(f\"False Positives (Low Stress predicted as High):  {cm[0][1]}\")\n",
        "print(f\"False Negatives (High Stress predicted as Low):  {cm[1][0]}\")\n",
        "print(f\"True Positives (High Stress correctly predicted): {cm[1][1]}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Plot Training History\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Plot accuracy\n",
        "ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "ax1.set_title('Model Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Plot loss\n",
        "ax2.plot(history.history['loss'], label='Training Loss')\n",
        "ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
        "ax2.set_title('Model Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print best validation accuracy\n",
        "best_val_acc = max(history.history['val_accuracy'])\n",
        "best_val_epoch = np.argmax(history.history['val_accuracy']) + 1\n",
        "print(f\"\\nBest Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%) at epoch {best_val_epoch}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Model and Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate best validation accuracy if not already calculated\n",
        "if 'best_val_acc' not in locals():\n",
        "    best_val_acc = max(history.history['val_accuracy'])\n",
        "    best_val_epoch = np.argmax(history.history['val_accuracy']) + 1\n",
        "\n",
        "# Save the trained model\n",
        "model.save('lstm_stress_model.h5')\n",
        "print(\"Model saved as 'lstm_stress_model.h5'\")\n",
        "\n",
        "# Save training history\n",
        "import pickle\n",
        "with open('lstm_training_history.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "print(\"Training history saved as 'lstm_training_history.pkl'\")\n",
        "\n",
        "# Save scaler for future use\n",
        "import joblib\n",
        "joblib.dump(scaler, 'lstm_scaler.pkl')\n",
        "print(\"Scaler saved as 'lstm_scaler.pkl'\")\n",
        "\n",
        "# Save results summary\n",
        "results_summary = {\n",
        "    'test_accuracy': float(test_accuracy),\n",
        "    'test_loss': float(test_loss),\n",
        "    'best_val_accuracy': float(best_val_acc),\n",
        "    'best_val_epoch': int(best_val_epoch),\n",
        "    'confusion_matrix': cm.tolist(),\n",
        "    'classification_report': metrics.classification_report(y_true, y_pred, \n",
        "                                                          target_names=['Low Stress', 'High Stress'], \n",
        "                                                          output_dict=True)\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('lstm_results.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=4)\n",
        "print(\"Results summary saved as 'lstm_results.json'\")\n",
        "print(\"\\nAll files saved successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
